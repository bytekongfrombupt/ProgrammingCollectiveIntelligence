{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 发现群组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用feedparser解析RSS订阅源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import feedparser\n",
    "# import re\n",
    "\n",
    "# # Returns title and dictionary of word counts for an RSS feed\n",
    "# def getwordcounts(url:str) -> (str, list):\n",
    "#   # Parse the feed\n",
    "#   d = feedparser.parse(url)\n",
    "#   wc = {}\n",
    "\n",
    "#   # Loop over all the entries\n",
    "#   for e in d.entries:\n",
    "#     if 'summary' in e: \n",
    "#         summary=e.summary\n",
    "#     else: \n",
    "#         summary=e.description\n",
    "\n",
    "#     # Extract a list of words\n",
    "#     words=getwords(e.title+' '+summary)\n",
    "#     for word in words:\n",
    "#       wc.setdefault(word,0)\n",
    "#       wc[word]+=1\n",
    "#   return d.feed.title, wc\n",
    "\n",
    "# def getwords(html:str) -> list:\n",
    "#   # Remove all the HTML tags\n",
    "#   txt=re.compile(r'<[^>]+>').sub('',html)\n",
    "\n",
    "#   # Split words by all non-alpha characters\n",
    "#   words=re.compile(r'[^A-Z^a-z]+').split(txt)\n",
    "\n",
    "#   # Convert to lowercase\n",
    "#   return [word.lower() for word in words if word!='']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析并准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apcount = {}\n",
    "# wordcounts = {}\n",
    "# feedlist = [line for line in open('./data/feedlist.txt')]\n",
    "# for feedurl in feedlist:\n",
    "#   try:\n",
    "#     title, wc = getwordcounts(feedurl)\n",
    "#     wordcounts[title] = wc\n",
    "#     for word, count in wc.items():\n",
    "#       apcount.setdefault(word, 0)\n",
    "#       if count > 1:\n",
    "#         apcount[word] += 1\n",
    "#   except:\n",
    "#     print('Failed to parse feed %s' % feedurl)\n",
    "\n",
    "# wordlist = []\n",
    "# for w, bc in apcount.items():\n",
    "#   frac=float(bc)/len(feedlist)\n",
    "#   if frac > 0.1 and frac < 0.5:\n",
    "#     wordlist.append(w)\n",
    "\n",
    "# out = open('./data/blogdata1.txt','w')\n",
    "# out.write('Blog')\n",
    "# for word in wordlist: \n",
    "#   out.write('\\t%s' % word)\n",
    "  \n",
    "# out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readfile(filename:str)->(list, list, list):\n",
    "  lines=[line for line in open(filename)]\n",
    "  \n",
    "  # First line is the column titles\n",
    "  colnames=lines[0].strip().split('\\t')[1:]\n",
    "  rownames=[]\n",
    "  data=[]\n",
    "  for line in lines[1:]:\n",
    "    p=line.strip().split('\\t')\n",
    "    # First column in each row is the rowname\n",
    "    rownames.append(p[0])\n",
    "    # The data for this row is the remainder of the row\n",
    "    data.append([float(x) for x in p[1:]])\n",
    "  return rownames,colnames,data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 皮尔逊相关度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def pearson(v1:list, v2:list) -> float:\n",
    "  # Simple sums\n",
    "  sum1=sum(v1)\n",
    "  sum2=sum(v2)\n",
    "  \n",
    "  # Sums of the squares\n",
    "  sum1Sq=sum([pow(v,2) for v in v1])\n",
    "  sum2Sq=sum([pow(v,2) for v in v2])\t\n",
    "  \n",
    "  # Sum of the products\n",
    "  pSum=sum([v1[i]*v2[i] for i in range(len(v1))])\n",
    "  \n",
    "  # Calculate r (Pearson score)\n",
    "  num=pSum-(sum1*sum2/len(v1))\n",
    "  den=sqrt((sum1Sq-pow(sum1,2)/len(v1))*(sum2Sq-pow(sum2,2)/len(v1)))\n",
    "  if den==0: return 0\n",
    "\n",
    "  return 1.0-num/den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义聚类class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bicluster:\n",
    "  def __init__(self,vec,left=None,right=None,distance=0.0,id=None):\n",
    "    self.left=left\n",
    "    self.right=right\n",
    "    self.vec=vec\n",
    "    self.id=id\n",
    "    self.distance=distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分级聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def hcluster(rows:list, distance:Callable) -> bicluster:\n",
    "  distances = {}\n",
    "  currentclustid = -1\n",
    "\n",
    "  # Clusters are initially just the rows\n",
    "  clust=[bicluster(rows[i], id=i) for i in range(len(rows))]\n",
    "\n",
    "  while len(clust)>1:\n",
    "    lowestpair=(0,1)\n",
    "    closest=distance(clust[0].vec,clust[1].vec)\n",
    "\n",
    "    # loop through every pair looking for the smallest distance\n",
    "    for i in range(len(clust)):\n",
    "      for j in range(i+1,len(clust)):\n",
    "        # distances is the cache of distance calculations\n",
    "        if (clust[i].id,clust[j].id) not in distances: \n",
    "          distances[(clust[i].id,clust[j].id)]=distance(clust[i].vec,clust[j].vec)\n",
    "\n",
    "        d=distances[(clust[i].id,clust[j].id)]\n",
    "\n",
    "        if d<closest:\n",
    "          closest=d\n",
    "          lowestpair=(i,j)\n",
    "\n",
    "    # calculate the average of the two clusters\n",
    "    mergevec=[\n",
    "    (clust[lowestpair[0]].vec[i]+clust[lowestpair[1]].vec[i])/2.0 \n",
    "    for i in range(len(clust[0].vec))]\n",
    "\n",
    "    # create the new cluster\n",
    "    newcluster=bicluster(mergevec,left=clust[lowestpair[0]],\n",
    "                         right=clust[lowestpair[1]],\n",
    "                         distance=closest,id=currentclustid)\n",
    "\n",
    "    # cluster ids that weren't in the original set are negative\n",
    "    currentclustid-=1\n",
    "    del clust[lowestpair[1]]\n",
    "    del clust[lowestpair[0]]\n",
    "    clust.append(newcluster)\n",
    "\n",
    "  return clust[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blognames, words, data = readfile('./data/blogdata.txt')\n",
    "clust = hcluster(data, pearson)\n",
    "#printclust(clust, labels=blognames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打印分级聚类层级结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printclust(clust:bicluster, labels=None, n=0):\n",
    "  # indent to make a hierarchy layout\n",
    "  for i in range(n): print (' ', end='')\n",
    "  if clust.id < 0:\n",
    "    # negative id means that this is branch\n",
    "    print ('-')\n",
    "  else:\n",
    "    # positive id means that this is an endpoint\n",
    "    if labels == None: print (clust.id)\n",
    "    else: print (labels[clust.id])\n",
    "\n",
    "  # now print the right and left branches\n",
    "  if clust.left!=None: printclust(clust.left,labels=labels,n=n+1)\n",
    "  if clust.right!=None: printclust(clust.right,labels=labels,n=n+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      " -\n",
      "  Seth Godin's Blog on marketing, tribes and respect\n",
      "  Autoblog\n",
      " -\n",
      "  -\n",
      "   SpikedHumor - Today's Videos and Pictures\n",
      "   -\n",
      "    -\n",
      "     Captain's Quarters\n",
      "     -\n",
      "      TMZ.com\n",
      "      PerezHilton\n",
      "    -\n",
      "     Steve Pavlina\n",
      "     -\n",
      "      -\n",
      "       Joho the Blog\n",
      "       BuzzMachine\n",
      "      -\n",
      "       Lifehack\n",
      "       -\n",
      "        Quick Online Tips\n",
      "        -\n",
      "         Copyblogger\n",
      "         -\n",
      "          TechCrunch\n",
      "          -\n",
      "           Wired\n",
      "           -\n",
      "            Engadget RSS Feed\n",
      "            -\n",
      "             -\n",
      "              Search Engine Roundtable\n",
      "              Google Operating System\n",
      "             -\n",
      "              The Official Google Blog\n",
      "              -\n",
      "               Schneier on Security\n",
      "               -\n",
      "                ShoeMoney\n",
      "                -\n",
      "                 mezzoblue\n",
      "                 -\n",
      "                  Scobleizer\n",
      "                  -\n",
      "                   blog maverick\n",
      "                   -\n",
      "                    -\n",
      "                     -\n",
      "                      Gothamist\n",
      "                      -\n",
      "                       The Full Feed from HuffingtonPost.com\n",
      "                       -\n",
      "                        Joel on Software\n",
      "                        -\n",
      "                         Kotaku\n",
      "                         -\n",
      "                          Lifehacker\n",
      "                          -\n",
      "                           Deadspin\n",
      "                           Gizmodo\n",
      "                     -\n",
      "                      Slashdot\n",
      "                      -\n",
      "                       MetaFilter\n",
      "                       -\n",
      "                        Eschaton\n",
      "                        -\n",
      "                         Latest from Crooks and Liars\n",
      "                         -\n",
      "                          Boing Boing\n",
      "                          -\n",
      "                           ThinkProgress - Medium\n",
      "                           -\n",
      "                            Daily Kos\n",
      "                            -\n",
      "                             null\n",
      "                             ProBlogger\n",
      "                    -\n",
      "                     plasticbag.org\n",
      "                     -\n",
      "                      Matt Cutts: Gadgets, Google, and SEO\n",
      "                      -\n",
      "                       456 Berea Street\n",
      "                       -\n",
      "                        -\n",
      "                         Neil Gaiman's Journal\n",
      "                         kottke.org\n",
      "                        -\n",
      "                         The Dish\n",
      "                         -\n",
      "                          Oilman\n",
      "                          -\n",
      "                           ongoing by Tim Bray\n",
      "                           -\n",
      "                            -\n",
      "                             WIL WHEATON dot NET\n",
      "                             Derek Powazek\n",
      "                            -\n",
      "                             Signal v. Noise - Medium\n",
      "                             -\n",
      "                              43 Folders\n",
      "                              PaulStamatiou.com - Technology, Design and Photography\n",
      "  -\n",
      "   O'Reilly Radar\n",
      "   -\n",
      "    Celebslam\n",
      "    The Write News\n"
     ]
    }
   ],
   "source": [
    "printclust(clust, labels=blognames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 绘制分级聚类的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def getheight(clust):\n",
    "  # Is this an endpoint? Then the height is just 1\n",
    "  if clust.left==None and clust.right==None: return 1\n",
    "\n",
    "  # Otherwise the height is the same of the heights of\n",
    "  # each branch\n",
    "  return getheight(clust.left)+getheight(clust.right)\n",
    "\n",
    "def getdepth(clust):\n",
    "  # The distance of an endpoint is 0.0\n",
    "  if clust.left==None and clust.right==None: return 0\n",
    "\n",
    "  # The distance of a branch is the greater of its two sides\n",
    "  # plus its own distance\n",
    "  return max(getdepth(clust.left),getdepth(clust.right))+clust.distance\n",
    "\n",
    "\n",
    "def drawdendrogram(clust,labels,jpeg='./outputs/clusters.jpg'):\n",
    "  # height and width\n",
    "  h=getheight(clust)*20\n",
    "  w=1200\n",
    "  depth=getdepth(clust)\n",
    "\n",
    "  # width is fixed, so scale distances accordingly\n",
    "  scaling=float(w-150)/depth\n",
    "\n",
    "  # Create a new image with a white background\n",
    "  img=Image.new('RGB',(w,h),(255,255,255))\n",
    "  draw=ImageDraw.Draw(img)\n",
    "\n",
    "  draw.line((0,h/2,10,h/2),fill=(255,0,0))    \n",
    "\n",
    "  # Draw the first node\n",
    "  drawnode(draw,clust,10,(h/2),scaling,labels)\n",
    "  img.save(jpeg,'JPEG')\n",
    "\n",
    "def drawnode(draw,clust,x,y,scaling,labels):\n",
    "  if clust.id<0:\n",
    "    h1=getheight(clust.left)*20\n",
    "    h2=getheight(clust.right)*20\n",
    "    top=y-(h1+h2)/2\n",
    "    bottom=y+(h1+h2)/2\n",
    "    # Line length\n",
    "    ll=clust.distance*scaling\n",
    "    # Vertical line from this cluster to children    \n",
    "    draw.line((x,top+h1/2,x,bottom-h2/2),fill=(255,0,0))    \n",
    "    \n",
    "    # Horizontal line to left item\n",
    "    draw.line((x,top+h1/2,x+ll,top+h1/2),fill=(255,0,0))    \n",
    "\n",
    "    # Horizontal line to right item\n",
    "    draw.line((x,bottom-h2/2,x+ll,bottom-h2/2),fill=(255,0,0))        \n",
    "\n",
    "    # Call the function to draw the left and right nodes    \n",
    "    drawnode(draw,clust.left,x+ll,top+h1/2,scaling,labels)\n",
    "    drawnode(draw,clust.right,x+ll,bottom-h2/2,scaling,labels)\n",
    "  else:   \n",
    "    # If this is an endpoint, draw the item label\n",
    "    draw.text((x+5,y-7),labels[clust.id],(0,0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图片保存至本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drawdendrogram(clust, blognames, jpeg='./outputs/blogclust.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 行和列对调，对单词聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rotatematrix(data:list) -> list:\n",
    "  newdata=[]\n",
    "  for i in range(len(data[0])):\n",
    "    newrow=[data[j][i] for j in range(len(data))]\n",
    "    newdata.append(newrow)\n",
    "  return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdata = rotatematrix(data)\n",
    "wordclust = hcluster(rdata, pearson)\n",
    "drawdendrogram(wordclust, labels=words, jpeg='./outputs/wordclust.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k均值聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def kcluster(rows, distance=pearson, k=4):\n",
    "\n",
    "  # Determine the minimum and maximum values for each point\n",
    "  ranges = [\n",
    "    (min([row[i] for row in rows]), max([row[i] for row in rows])) \n",
    "    for i in range(len(rows[0]))\n",
    "  ]\n",
    "\n",
    "  # Create k randomly placed centroids\n",
    "  clusters=[\n",
    "    [\n",
    "      random.random()*(ranges[i][1]-ranges[i][0])+ranges[i][0] \n",
    "      for i in range(len(rows[0]))\n",
    "    ] \n",
    "    for _ in range(k)\n",
    "  ]\n",
    "  \n",
    "  lastmatches = None\n",
    "  for t in range(100):\n",
    "    print('Iteration %d' % t)\n",
    "    bestmatches=[ [] for _ in range(k) ]\n",
    "    \n",
    "    # Find which centroid is the closest for each row\n",
    "    for j in range(len(rows)):\n",
    "      row=rows[j]\n",
    "      bestmatch=0\n",
    "      for i in range(k):\n",
    "        d=distance(clusters[i],row)\n",
    "        if d<distance(clusters[bestmatch],row): bestmatch=i\n",
    "      bestmatches[bestmatch].append(j)\n",
    "\n",
    "    # If the results are the same as last time, this is complete\n",
    "    if bestmatches==lastmatches: break\n",
    "    lastmatches=bestmatches\n",
    "    \n",
    "    # Move the centroids to the average of their members\n",
    "    for i in range(k):\n",
    "      avgs=[0.0]*len(rows[0])\n",
    "      if len(bestmatches[i])>0:\n",
    "        for rowid in bestmatches[i]:\n",
    "          for m in range(len(rows[rowid])):\n",
    "            avgs[m]+=rows[rowid][m]\n",
    "        for j in range(len(avgs)):\n",
    "          avgs[j]/=len(bestmatches[i])\n",
    "        clusters[i]=avgs\n",
    "      \n",
    "  return bestmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "[[5, 7, 10, 11, 12, 15, 18, 19, 21, 23, 24, 27, 38, 40, 44, 45, 47, 52, 53, 54], [8, 13, 14, 16, 20, 22, 25, 26, 28, 29, 31, 32, 33, 34, 36, 39, 49], [0, 1, 3, 4, 6, 17, 30, 35, 43, 46, 50, 51], [2, 9, 37, 41, 42, 48]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['TechCrunch',\n",
       " 'Autoblog',\n",
       " 'Kotaku',\n",
       " \"SpikedHumor - Today's Videos and Pictures\",\n",
       " \"O'Reilly Radar\",\n",
       " 'Joel on Software',\n",
       " 'The Full Feed from HuffingtonPost.com',\n",
       " 'Celebslam',\n",
       " 'Lifehacker',\n",
       " 'The Official Google Blog',\n",
       " 'The Write News',\n",
       " 'Deadspin',\n",
       " 'Gizmodo',\n",
       " 'Engadget RSS Feed',\n",
       " 'Slashdot',\n",
       " 'Copyblogger',\n",
       " 'Gothamist']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kclust=kcluster(data, k=4)\n",
    "print(kclust)\n",
    "[blognames[r] for r in kclust[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 针对偏好的聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from BeautifulSoup import BeautifulSoup\n",
    "# import urllib2\n",
    "# import re\n",
    "# chare=re.compile(r'[!-\\.&]')\n",
    "# itemowners={}\n",
    "\n",
    "# # Words to remove\n",
    "# dropwords=['a','new','some','more','my','own','the','many','other','another']\n",
    "\n",
    "# currentuser=0\n",
    "# for i in range(1,51):\n",
    "#   # URL for the want search page\n",
    "#   c=urllib2.urlopen(\n",
    "#   'http://member.zebo.com/Main?event_key=USERSEARCH&wiowiw=wiw&keyword=car&page=%d'\n",
    "#   % (i))\n",
    "#   soup=BeautifulSoup(c.read())\n",
    "#   for td in soup('td'):\n",
    "#     # Find table cells of bgverdanasmall class\n",
    "#     if ('class' in dict(td.attrs) and td['class']=='bgverdanasmall'):\n",
    "#       items=[re.sub(chare,'',str(a.contents[0]).lower()).strip() for a in td('a')]\n",
    "#       for item in items:\n",
    "#         # Remove extra words\n",
    "#         txt=' '.join([t for t in item.split(' ') if t not in dropwords])\n",
    "#         if len(txt)<2: continue\n",
    "#         itemowners.setdefault(txt,{})\n",
    "#         itemowners[txt][currentuser]=1\n",
    "#       currentuser+=1\n",
    "      \n",
    "# out=open('zebo.txt','w')\n",
    "# out.write('Item')\n",
    "# for user in range(0,currentuser): out.write('\\tU%d' % user)\n",
    "# out.write('\\n')\n",
    "# for item,owners in itemowners.items():\n",
    "#   if len(owners)>10:\n",
    "#     out.write(item)\n",
    "#     for user in range(0,currentuser):\n",
    "#       if user in owners: out.write('\\t1')\n",
    "#       else: out.write('\\t0')\n",
    "#     out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pearson系数适合上述博客的统治数值，而此处偏好数据只有0和1两种取值。\n",
    "# 对偏好进行分析时，对同时希望拥有两件物品的人在物品方面互有重叠情况的度量是更有意义的，所以这里使用tanimoto系数，它代表的是交集与并集的比率\n",
    "def tanamoto(v1:list,v2:list) -> float:\n",
    "  c1,c2,shr=0,0,0\n",
    "  \n",
    "  for i in range(len(v1)):\n",
    "    if v1[i]!=0: c1+=1 # in v1\n",
    "    if v2[i]!=0: c2+=1 # in v2\n",
    "    if v1[i]!=0 and v2[i]!=0: shr+=1 # in both\n",
    "  \n",
    "  return 1.0-(float(shr)/(c1+c2-shr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wants, people, data = readfile('./data/zebo.txt')\n",
    "clust = hcluster(data, distance=tanamoto)\n",
    "drawdendrogram(clust, wants, jpeg='./outputs/itemclust.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scaledown(data,distance=pearson,rate=0.01):\n",
    "  n = len(data)\n",
    "\n",
    "  # The real distances between every pair of items\n",
    "  realdist = [\n",
    "    [\n",
    "      distance(data[i],data[j]) for j in range(n)\n",
    "    ] \n",
    "    for i in range(0,n)\n",
    "  ]\n",
    "  \n",
    "  # Randomly initialize the starting points of the locations in 2D\n",
    "  loc = [[random.random(),random.random()] for i in range(n)]\n",
    "  fakedist=[[0.0 for j in range(n)] for i in range(n)]\n",
    "  \n",
    "  lasterror=None\n",
    "  for m in range(0,1000):\n",
    "    # Find projected distances\n",
    "    for i in range(n):\n",
    "      for j in range(n):\n",
    "        fakedist[i][j]=sqrt(\n",
    "          sum([pow(loc[i][x]-loc[j][x],2) for x in range(len(loc[i]))])\n",
    "        )\n",
    "  \n",
    "    # Move points\n",
    "    grad=[[0.0,0.0] for i in range(n)]\n",
    "    \n",
    "    totalerror=0\n",
    "    for k in range(n):\n",
    "      for j in range(n):\n",
    "        if j==k: continue\n",
    "        # The error is percent difference between the distances\n",
    "        devided_by = realdist[j][k] + 1 if realdist[j][k] == 0 else realdist[j][k] # Avoid devided by Zero\n",
    "        errorterm=(fakedist[j][k]-realdist[j][k])/(devided_by) \n",
    "        \n",
    "        # Each point needs to be moved away from or towards the other\n",
    "        # point in proportion to how much error it has\n",
    "        devided_by = fakedist[j][k] + 1 if fakedist[j][k] == 0 else fakedist[j][k] # Avoid devided by Zero\n",
    "        grad[k][0]+=((loc[k][0]-loc[j][0])/fakedist[j][k])*errorterm\n",
    "        grad[k][1]+=((loc[k][1]-loc[j][1])/fakedist[j][k])*errorterm\n",
    "\n",
    "        # Keep track of the total error\n",
    "        totalerror+=abs(errorterm)\n",
    "    print (totalerror)\n",
    "\n",
    "    # If the answer got worse by moving the points, we are done\n",
    "    if lasterror and lasterror<totalerror: break\n",
    "    lasterror=totalerror\n",
    "    \n",
    "    # Move each of the points by the learning rate times the gradient\n",
    "    for k in range(n):\n",
    "      loc[k][0]-=rate*grad[k][0]\n",
    "      loc[k][1]-=rate*grad[k][1]\n",
    "\n",
    "  return loc\n",
    "\n",
    "def draw2d(data,labels,jpeg='mds2d.jpg'):\n",
    "  img=Image.new('RGB',(2000,2000),(255,255,255))\n",
    "  draw=ImageDraw.Draw(img)\n",
    "  for i in range(len(data)):\n",
    "    x=(data[i][0]+0.5)*1000\n",
    "    y=(data[i][1]+0.5)*1000\n",
    "    draw.text((x,y),labels[i],(0,0,0))\n",
    "  img.save(jpeg,'JPEG')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1426.508974360985\n",
      "1193.4753302719791\n",
      "1061.3521187452052\n",
      "992.3888289626459\n",
      "956.3097881612326\n",
      "931.3806365102797\n",
      "915.2060576996757\n",
      "902.7810309579789\n",
      "893.3517499689957\n",
      "884.5263270705593\n",
      "876.2067843786449\n",
      "867.9858608190254\n",
      "860.4207741547485\n",
      "852.6459294384755\n",
      "845.0499823122416\n",
      "837.5872163470021\n",
      "830.51798133836\n",
      "823.6182556738404\n",
      "817.7125172193718\n",
      "812.0220558039597\n",
      "806.5245786265278\n",
      "800.6047589929608\n",
      "795.1765769071549\n",
      "790.479501225291\n",
      "786.4835909897384\n",
      "783.0336181943853\n",
      "780.3297706984348\n",
      "778.0562684302722\n",
      "776.4720713528421\n",
      "775.0221013538263\n",
      "773.5182495477893\n",
      "772.0148547044402\n",
      "770.1807669353827\n",
      "768.1788240565552\n",
      "766.104742534446\n",
      "764.1245253550144\n",
      "762.2648916407103\n",
      "760.605073724132\n",
      "759.1689070897276\n",
      "758.0126317969496\n",
      "756.9857943840668\n",
      "756.0256856191274\n",
      "754.9943355927517\n",
      "753.9568822362374\n",
      "752.9247502742214\n",
      "752.0433078333236\n",
      "751.2926507919886\n",
      "750.8386183695202\n",
      "750.4355081208022\n",
      "750.0197851856263\n",
      "749.6218821481544\n",
      "749.1334860130177\n",
      "748.5995952268464\n",
      "747.9945485703873\n",
      "747.4338465812879\n",
      "746.8602335583395\n",
      "746.2954367671344\n",
      "745.8657371372747\n",
      "745.5273816523408\n",
      "745.1650723116344\n",
      "744.8020547153143\n",
      "744.4142613346256\n",
      "744.0039119245793\n",
      "743.6636801929416\n",
      "743.2996230736217\n",
      "742.9275487448642\n",
      "742.621271235026\n",
      "742.2950094282825\n",
      "741.9779461157251\n",
      "741.7040571913252\n",
      "741.4761723808052\n",
      "741.333812160286\n",
      "741.2068219312988\n",
      "741.067072730742\n",
      "740.9238077214643\n",
      "740.7949993078279\n",
      "740.6663463247602\n",
      "740.5490726630156\n",
      "740.440328891652\n",
      "740.3344710238927\n",
      "740.2323296634095\n",
      "740.1301504783485\n",
      "740.029183425592\n",
      "739.9334425043409\n",
      "739.8375687464272\n",
      "739.7555017887378\n",
      "739.6955529546981\n",
      "739.6364555440017\n",
      "739.5768355087098\n",
      "739.5175195683084\n",
      "739.4579625584048\n",
      "739.3970709458968\n",
      "739.3348281699542\n",
      "739.2712039021509\n",
      "739.2069209856475\n",
      "739.1446847314087\n",
      "739.0823988239512\n",
      "739.0211584713127\n",
      "738.9632529453836\n",
      "738.9065053592575\n",
      "738.8517137847729\n",
      "738.7943717443876\n",
      "738.7343626667679\n",
      "738.6715304839615\n",
      "738.6098070300951\n",
      "738.5451578153674\n",
      "738.4773979284384\n",
      "738.4173746964423\n",
      "738.3605241975342\n",
      "738.3012834748255\n",
      "738.2402622470254\n",
      "738.1729140604378\n",
      "738.1038101115521\n",
      "738.0277436945786\n",
      "737.9383205386466\n",
      "737.8310001666198\n",
      "737.6968493551449\n",
      "737.5264421076134\n",
      "737.3283468585745\n",
      "737.1137990943402\n",
      "736.8456677684521\n",
      "736.5531887642084\n",
      "736.292924192641\n",
      "736.1200509300135\n",
      "736.0334549167056\n",
      "735.9983152166735\n",
      "735.9264600284547\n",
      "735.8099294516835\n",
      "735.684789434831\n",
      "735.6068464798095\n",
      "735.4602696451008\n",
      "735.3090787562508\n",
      "735.1819544993415\n",
      "735.0246957476413\n",
      "734.8442055417962\n",
      "734.6373379182953\n",
      "734.4350059301402\n",
      "734.2173598383229\n",
      "733.9644719259147\n",
      "733.6843340573733\n",
      "733.4287271041276\n",
      "733.1870500108007\n",
      "732.966811154736\n",
      "732.7531177324676\n",
      "732.5528586242591\n",
      "732.3664266082044\n",
      "732.196992216616\n",
      "732.0460070636032\n",
      "731.9117951189664\n",
      "731.782187901271\n",
      "731.6553184693337\n",
      "731.5375502999664\n",
      "731.4222518833931\n",
      "731.3064883926614\n",
      "731.204717884301\n",
      "731.102434759052\n",
      "730.9996624854772\n",
      "730.8963891674242\n",
      "730.7925707513238\n",
      "730.6883245817237\n",
      "730.5876604104753\n",
      "730.5074797442261\n",
      "730.4298951608616\n",
      "730.3521988053989\n",
      "730.2732743868314\n",
      "730.1936249452112\n",
      "730.1166468762426\n",
      "730.0369457817894\n",
      "729.9540074494213\n",
      "729.8672338815315\n",
      "729.7759434012814\n",
      "729.6793801499892\n",
      "729.584065414187\n",
      "729.5015123503956\n",
      "729.4154991215134\n",
      "729.3238712688759\n",
      "729.2260600324631\n",
      "729.1277275570361\n",
      "729.033448315034\n",
      "728.9464864622382\n",
      "728.8614212136549\n",
      "728.778743400662\n",
      "728.6994122028817\n",
      "728.629194586213\n",
      "728.5566602721591\n",
      "728.4878609265454\n",
      "728.4183424675145\n",
      "728.3550634999377\n",
      "728.3104035860977\n",
      "728.2649146168873\n",
      "728.2133416353776\n",
      "728.15016514145\n",
      "728.0682494692044\n",
      "727.9152577056086\n",
      "727.6578284914781\n",
      "727.2514122408106\n",
      "726.6932230868648\n",
      "726.0604728215736\n",
      "725.4847440808164\n",
      "724.9503629719543\n",
      "724.5575417289049\n",
      "724.2411966237785\n",
      "724.0627911449182\n",
      "723.9192495326256\n",
      "723.8173358348903\n",
      "723.7308677370871\n",
      "723.65087363217\n",
      "723.5726843765128\n",
      "723.4910789552933\n",
      "723.4099099668409\n",
      "723.327816670805\n",
      "723.2451311617168\n",
      "723.1646385784378\n",
      "723.0903701913122\n",
      "723.0193561481735\n",
      "722.9493841600381\n",
      "722.8807268289785\n",
      "722.8140181190373\n",
      "722.7518340363886\n",
      "722.6911228417175\n",
      "722.6320460798993\n",
      "722.5748109365414\n",
      "722.5313384949266\n",
      "722.4935985997433\n",
      "722.4570225259441\n",
      "722.4216355163865\n",
      "722.3874381582228\n",
      "722.3586481008347\n",
      "722.3376629493877\n",
      "722.3187910939147\n",
      "722.3027401292807\n",
      "722.287849620743\n",
      "722.2741517633797\n",
      "722.2650892345873\n",
      "722.257280170665\n",
      "722.2517375702189\n",
      "722.2463079784147\n",
      "722.2409724129723\n",
      "722.2357112632773\n",
      "722.2311835660092\n",
      "722.227545066142\n",
      "722.2239761000588\n",
      "722.2204535831474\n",
      "722.2169827891528\n",
      "722.2135791774261\n",
      "722.2136439166071\n"
     ]
    }
   ],
   "source": [
    "blognames, words, data = readfile('./data/blogdata.txt')\n",
    "coords = scaledown(data, rate=0.01)\n",
    "draw2d(coords, blognames, jpeg='./outputs/blogs2d2.jpg')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
